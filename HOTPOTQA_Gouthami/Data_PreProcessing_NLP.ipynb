{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary Installations section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "executionInfo": {
     "elapsed": 7358,
     "status": "ok",
     "timestamp": 1603574358581,
     "user": {
      "displayName": "Gouthami Kanchukatla",
      "photoUrl": "",
      "userId": "06998806167954433550"
     },
     "user_tz": 420
    },
    "id": "89yzY2GYTHgy",
    "outputId": "4d2359c8-0998-4afb-e492-5a3c6fcfc386"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/tharun/anaconda3/lib/python3.8/site-packages (3.4.0)\n",
      "Requirement already satisfied: packaging in /home/tharun/anaconda3/lib/python3.8/site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: sacremoses in /home/tharun/anaconda3/lib/python3.8/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /home/tharun/anaconda3/lib/python3.8/site-packages (from transformers) (0.1.94)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/tharun/anaconda3/lib/python3.8/site-packages (from transformers) (2020.6.8)\n",
      "Requirement already satisfied: filelock in /home/tharun/anaconda3/lib/python3.8/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tokenizers==0.9.2 in /home/tharun/anaconda3/lib/python3.8/site-packages (from transformers) (0.9.2)\n",
      "Requirement already satisfied: numpy in /home/tharun/anaconda3/lib/python3.8/site-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: requests in /home/tharun/anaconda3/lib/python3.8/site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: protobuf in /home/tharun/anaconda3/lib/python3.8/site-packages (from transformers) (3.13.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/tharun/anaconda3/lib/python3.8/site-packages (from transformers) (4.47.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/tharun/anaconda3/lib/python3.8/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: six in /home/tharun/anaconda3/lib/python3.8/site-packages (from packaging->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /home/tharun/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /home/tharun/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers) (0.16.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/tharun/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/tharun/anaconda3/lib/python3.8/site-packages (from requests->transformers) (1.25.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/tharun/anaconda3/lib/python3.8/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tharun/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: setuptools in /home/tharun/.local/lib/python3.8/site-packages (from protobuf->transformers) (50.3.0)\n",
      "Requirement already satisfied: unidecode in /home/tharun/anaconda3/lib/python3.8/site-packages (1.1.1)\n",
      "Requirement already satisfied: pandas in /home/tharun/anaconda3/lib/python3.8/site-packages (1.0.5)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/tharun/anaconda3/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/tharun/anaconda3/lib/python3.8/site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/tharun/anaconda3/lib/python3.8/site-packages (from pandas) (1.18.5)\n",
      "Requirement already satisfied: six>=1.5 in /home/tharun/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.6.1->pandas) (1.15.0)\n",
      "Requirement already satisfied: keras in /home/tharun/anaconda3/lib/python3.8/site-packages (2.4.3)\n",
      "Requirement already satisfied: h5py in /home/tharun/anaconda3/lib/python3.8/site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/tharun/anaconda3/lib/python3.8/site-packages (from keras) (1.18.5)\n",
      "Requirement already satisfied: pyyaml in /home/tharun/anaconda3/lib/python3.8/site-packages (from keras) (5.3.1)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/tharun/anaconda3/lib/python3.8/site-packages (from keras) (1.5.0)\n",
      "Requirement already satisfied: six in /home/tharun/anaconda3/lib/python3.8/site-packages (from h5py->keras) (1.15.0)\n",
      "Requirement already satisfied: tensorflow in /home/tharun/anaconda3/lib/python3.8/site-packages (2.3.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /home/tharun/anaconda3/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /home/tharun/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /home/tharun/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in /home/tharun/anaconda3/lib/python3.8/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/tharun/anaconda3/lib/python3.8/site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/tharun/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /home/tharun/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.18.5)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /home/tharun/anaconda3/lib/python3.8/site-packages (from tensorflow) (0.10.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/tharun/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /home/tharun/anaconda3/lib/python3.8/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /home/tharun/anaconda3/lib/python3.8/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/tharun/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/tharun/anaconda3/lib/python3.8/site-packages (from tensorflow) (0.34.2)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/tharun/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.33.1)\n",
      "Requirement already satisfied: gast==0.3.3 in /home/tharun/anaconda3/lib/python3.8/site-packages (from tensorflow) (0.3.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/tharun/anaconda3/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/tharun/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/tharun/.local/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (50.3.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/tharun/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.22.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/tharun/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/tharun/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/tharun/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/tharun/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.24.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/tharun/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in /home/tharun/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/tharun/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/tharun/anaconda3/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tharun/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/tharun/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.25.10)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/tharun/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/tharun/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/tharun/anaconda3/lib/python3.8/site-packages (from rsa<5,>=3.1.4; python_version >= \"3.5\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/tharun/anaconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/tharun/anaconda3/lib/python3.8/site-packages (1.6.0)\n",
      "Requirement already satisfied: numpy in /home/tharun/anaconda3/lib/python3.8/site-packages (from torch) (1.18.5)\n",
      "Requirement already satisfied: future in /home/tharun/anaconda3/lib/python3.8/site-packages (from torch) (0.18.2)\n",
      "Collecting git+https://github.com/AndriyMulyar/semantic-text-similarity\n",
      "  Cloning https://github.com/AndriyMulyar/semantic-text-similarity to /tmp/pip-req-build-zrf5husd\n",
      "  Running command git clone -q https://github.com/AndriyMulyar/semantic-text-similarity /tmp/pip-req-build-zrf5husd\n",
      "Requirement already satisfied (use --upgrade to upgrade): semantic-text-similarity==1.0.3 from git+https://github.com/AndriyMulyar/semantic-text-similarity in /home/tharun/anaconda3/lib/python3.8/site-packages\n",
      "Requirement already satisfied: torch in /home/tharun/anaconda3/lib/python3.8/site-packages (from semantic-text-similarity==1.0.3) (1.6.0)\n",
      "Requirement already satisfied: strsim in /home/tharun/anaconda3/lib/python3.8/site-packages (from semantic-text-similarity==1.0.3) (0.0.3)\n",
      "Requirement already satisfied: fuzzywuzzy[speedup] in /home/tharun/anaconda3/lib/python3.8/site-packages (from semantic-text-similarity==1.0.3) (0.18.0)\n",
      "Requirement already satisfied: pytorch-transformers==1.1.0 in /home/tharun/anaconda3/lib/python3.8/site-packages (from semantic-text-similarity==1.0.3) (1.1.0)\n",
      "Requirement already satisfied: scipy in /home/tharun/anaconda3/lib/python3.8/site-packages (from semantic-text-similarity==1.0.3) (1.5.0)\n",
      "Requirement already satisfied: future in /home/tharun/anaconda3/lib/python3.8/site-packages (from torch->semantic-text-similarity==1.0.3) (0.18.2)\n",
      "Requirement already satisfied: numpy in /home/tharun/anaconda3/lib/python3.8/site-packages (from torch->semantic-text-similarity==1.0.3) (1.18.5)\n",
      "Requirement already satisfied: python-levenshtein>=0.12; extra == \"speedup\" in /home/tharun/anaconda3/lib/python3.8/site-packages (from fuzzywuzzy[speedup]->semantic-text-similarity==1.0.3) (0.12.0)\n",
      "Requirement already satisfied: regex in /home/tharun/anaconda3/lib/python3.8/site-packages (from pytorch-transformers==1.1.0->semantic-text-similarity==1.0.3) (2020.6.8)\n",
      "Requirement already satisfied: requests in /home/tharun/anaconda3/lib/python3.8/site-packages (from pytorch-transformers==1.1.0->semantic-text-similarity==1.0.3) (2.24.0)\n",
      "Requirement already satisfied: tqdm in /home/tharun/anaconda3/lib/python3.8/site-packages (from pytorch-transformers==1.1.0->semantic-text-similarity==1.0.3) (4.47.0)\n",
      "Requirement already satisfied: sentencepiece in /home/tharun/anaconda3/lib/python3.8/site-packages (from pytorch-transformers==1.1.0->semantic-text-similarity==1.0.3) (0.1.94)\n",
      "Requirement already satisfied: boto3 in /home/tharun/anaconda3/lib/python3.8/site-packages (from pytorch-transformers==1.1.0->semantic-text-similarity==1.0.3) (1.16.4)\n",
      "Requirement already satisfied: setuptools in /home/tharun/.local/lib/python3.8/site-packages (from python-levenshtein>=0.12; extra == \"speedup\"->fuzzywuzzy[speedup]->semantic-text-similarity==1.0.3) (50.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tharun/anaconda3/lib/python3.8/site-packages (from requests->pytorch-transformers==1.1.0->semantic-text-similarity==1.0.3) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/tharun/anaconda3/lib/python3.8/site-packages (from requests->pytorch-transformers==1.1.0->semantic-text-similarity==1.0.3) (1.25.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/tharun/anaconda3/lib/python3.8/site-packages (from requests->pytorch-transformers==1.1.0->semantic-text-similarity==1.0.3) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/tharun/anaconda3/lib/python3.8/site-packages (from requests->pytorch-transformers==1.1.0->semantic-text-similarity==1.0.3) (2.10)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/tharun/anaconda3/lib/python3.8/site-packages (from boto3->pytorch-transformers==1.1.0->semantic-text-similarity==1.0.3) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.20.0,>=1.19.4 in /home/tharun/anaconda3/lib/python3.8/site-packages (from boto3->pytorch-transformers==1.1.0->semantic-text-similarity==1.0.3) (1.19.4)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/tharun/anaconda3/lib/python3.8/site-packages (from boto3->pytorch-transformers==1.1.0->semantic-text-similarity==1.0.3) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/tharun/anaconda3/lib/python3.8/site-packages (from botocore<1.20.0,>=1.19.4->boto3->pytorch-transformers==1.1.0->semantic-text-similarity==1.0.3) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/tharun/anaconda3/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.20.0,>=1.19.4->boto3->pytorch-transformers==1.1.0->semantic-text-similarity==1.0.3) (1.15.0)\n",
      "Building wheels for collected packages: semantic-text-similarity\n",
      "  Building wheel for semantic-text-similarity (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for semantic-text-similarity: filename=semantic_text_similarity-1.0.3-py3-none-any.whl size=416023 sha256=6fe74a2f759688899760a304a009dadab95550815e480250b27d79e9e71de901\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-j5v041x9/wheels/53/38/40/8492fa5fef9e81bbdf64927c1c1b8ef9b5ac39cfec09f526dc\n",
      "Successfully built semantic-text-similarity\n",
      "Requirement already satisfied: urllib3==1.25.10 in /home/tharun/anaconda3/lib/python3.8/site-packages (1.25.10)\n",
      "Requirement already satisfied: tqdm in /home/tharun/anaconda3/lib/python3.8/site-packages (4.47.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers\n",
    "!pip3 install unidecode\n",
    "!pip install pandas\n",
    "!pip install keras \n",
    "!pip install tensorflow\n",
    "!pip install torch\n",
    "!pip install git+https://github.com/AndriyMulyar/semantic-text-similarity\n",
    "!pip install urllib3==1.25.10\n",
    "!pip install tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "executionInfo": {
     "elapsed": 10713,
     "status": "ok",
     "timestamp": 1603574361943,
     "user": {
      "displayName": "Gouthami Kanchukatla",
      "photoUrl": "",
      "userId": "06998806167954433550"
     },
     "user_tz": 420
    },
    "id": "XgJirCHsZoO4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "import unidecode\n",
    "import re\n",
    "import logging\n",
    "from tqdm.notebook import tnrange\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "\n",
    "#For ploting results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# DL Libraries\n",
    "from transformers import BertModel, AdamW, BertTokenizer, BertConfig, get_linear_schedule_with_warmup\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,TensorDataset)\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# #NLTK Libraries\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "executionInfo": {
     "elapsed": 16514,
     "status": "ok",
     "timestamp": 1603574367802,
     "user": {
      "displayName": "Gouthami Kanchukatla",
      "photoUrl": "",
      "userId": "06998806167954433550"
     },
     "user_tz": 420
    },
    "id": "Btlpy-D9dYDL"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "# print(\"device: {} n_gpu: {}\".format(device, n_gpu))\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "# print(logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "executionInfo": {
     "elapsed": 20074,
     "status": "ok",
     "timestamp": 1603574371371,
     "user": {
      "displayName": "Gouthami Kanchukatla",
      "photoUrl": "",
      "userId": "06998806167954433550"
     },
     "user_tz": 420
    },
    "id": "hCzkGq8uJC0d",
    "outputId": "c86e22e7-aae1-45eb-fa29-ed94c0464625"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/24/2020 23:10:42 - INFO - pytorch_transformers.tokenization_utils -   Model name '/home/tharun/.cache/torch/semantic_text_similarity/448b457fca135ab38bb3d6af49c5f220a8167e6b6ce9012f7df8172aa29865f9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '/home/tharun/.cache/torch/semantic_text_similarity/448b457fca135ab38bb3d6af49c5f220a8167e6b6ce9012f7df8172aa29865f9' is a path or url to a directory containing tokenizer files.\n",
      "10/24/2020 23:10:42 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file /home/tharun/.cache/torch/semantic_text_similarity/448b457fca135ab38bb3d6af49c5f220a8167e6b6ce9012f7df8172aa29865f9/added_tokens.json. We won't load it.\n",
      "10/24/2020 23:10:42 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file /home/tharun/.cache/torch/semantic_text_similarity/448b457fca135ab38bb3d6af49c5f220a8167e6b6ce9012f7df8172aa29865f9/special_tokens_map.json. We won't load it.\n",
      "10/24/2020 23:10:42 - INFO - pytorch_transformers.tokenization_utils -   loading file /home/tharun/.cache/torch/semantic_text_similarity/448b457fca135ab38bb3d6af49c5f220a8167e6b6ce9012f7df8172aa29865f9/vocab.txt\n",
      "10/24/2020 23:10:42 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "10/24/2020 23:10:42 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "10/24/2020 23:10:42 - INFO - pytorch_transformers.modeling_utils -   loading weights file /home/tharun/.cache/torch/semantic_text_similarity/448b457fca135ab38bb3d6af49c5f220a8167e6b6ce9012f7df8172aa29865f9/pytorch_model.bin\n",
      "10/24/2020 23:10:44 - INFO - root -   Initialized BertSentencePairSimilarity model from /home/tharun/.cache/torch/semantic_text_similarity/448b457fca135ab38bb3d6af49c5f220a8167e6b6ce9012f7df8172aa29865f9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3.0079887], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from semantic_text_similarity.models import WebBertSimilarity\n",
    "\n",
    "web_model = WebBertSimilarity(device='cuda', batch_size=10) #defaults to GPU prediction\n",
    "\n",
    "#clinical_model = ClinicalBertSimilarity(device='cuda', batch_size=10) #defaults to GPU prediction\n",
    "\n",
    "web_model.predict([(\"She won an olympic gold medal\",\"The women is an olympic champion\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0kl-1vJRo6e"
   },
   "source": [
    "# Data Preprocessing section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kF-57dt5c_AO"
   },
   "source": [
    "Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "executionInfo": {
     "elapsed": 20072,
     "status": "ok",
     "timestamp": 1603574371372,
     "user": {
      "displayName": "Gouthami Kanchukatla",
      "photoUrl": "",
      "userId": "06998806167954433550"
     },
     "user_tz": 420
    },
    "id": "U4BvDcc8T7uW"
   },
   "outputs": [],
   "source": [
    "def generate_df(df, start,end):\n",
    "    new_df = pd.DataFrame(columns=['question-id','passage-id','query','passage_title','sentence', 'passage_len'])\n",
    "    pd.set_option('display.max_seq_items', None)\n",
    "    num_psges =  0\n",
    "    print(\"max length of the data : \",len(df))\n",
    "\n",
    "    for i in tnrange(start,end):\n",
    "        cntxt_len = len(df['context'][i])\n",
    "        num_psges  = num_psges + cntxt_len\n",
    "\n",
    "        # query = []\n",
    "        # query.append(df['question'][i])\n",
    "        # query.append(df['answer'][i])\n",
    "\n",
    "        query = str(df['question'][i] + df['answer'][i])\n",
    "        if len(query) < 2:\n",
    "            continue\n",
    "\n",
    "        for j  in range(0,cntxt_len):\n",
    "            senList = df['context'][i][j][1]\n",
    "            passage_title =  df['context'][i][j][0]\n",
    "            k = 1\n",
    "            for each_sen in senList:\n",
    "                # filtering empty sentences\n",
    "                if len(each_sen) < 2 or len(passage_title)< 2:\n",
    "                    continue\n",
    "\n",
    "                ques_id = str(i+1)\n",
    "                psge_id  =  str(j+1)\n",
    "                new_row = {'question-id':ques_id,'passage-id':psge_id, 'query':query,'passage_title': passage_title,'sentence': each_sen,'passage_len': len(senList)}\n",
    "                # print(new_row)\n",
    "                new_df =  new_df.append(new_row,ignore_index= True)\n",
    "    print(end,\"length=\", len(new_df))\n",
    "    return  new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "executionInfo": {
     "elapsed": 20070,
     "status": "ok",
     "timestamp": 1603574371372,
     "user": {
      "displayName": "Gouthami Kanchukatla",
      "photoUrl": "",
      "userId": "06998806167954433550"
     },
     "user_tz": 420
    },
    "id": "2CUqXgCZUGDQ"
   },
   "outputs": [],
   "source": [
    "def generate_files(inpfilename,  foldername, start,end, forcegenerate = False):\n",
    "    df = pd.read_json(inpfilename);\n",
    "    str1 = foldername \n",
    "    str3 = \".csv\"\n",
    "    fullfilename = \"\".join((str1,str(start), \"_\", str(end),str3))\n",
    "    \n",
    "    os.makedirs(os.path.dirname(fullfilename), exist_ok=True)\n",
    "\n",
    "    if os.path.isfile(fullfilename) and forcegenerate == False:\n",
    "        print(fullfilename + \" already  created -  so skipping  generation\")\n",
    "    else:\n",
    "        new_df  =  generate_df(df,start,end)\n",
    "        new_df.to_csv(fullfilename,index = False)\n",
    "        print(fullfilename + \" generated\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QqID-4q3dBWI"
   },
   "source": [
    "Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "executionInfo": {
     "elapsed": 444,
     "status": "ok",
     "timestamp": 1603576818683,
     "user": {
      "displayName": "Gouthami Kanchukatla",
      "photoUrl": "",
      "userId": "06998806167954433550"
     },
     "user_tz": 420
    },
    "id": "yqmICuDnYpsA"
   },
   "outputs": [],
   "source": [
    "\n",
    "start = 10\n",
    "end = 12\n",
    "FOLDER = '/home/tharun/Downloads/Gouthami/NLP_Project/Training_Data/Submission'\n",
    "\n",
    "JSON_FILE =  FOLDER +\"/hotpot_train_v1.1.json\"\n",
    "FILE_PREFIX  =  FOLDER +  '/parsed_df/parsed_data_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 7508,
     "status": "ok",
     "timestamp": 1603577240009,
     "user": {
      "displayName": "Gouthami Kanchukatla",
      "photoUrl": "",
      "userId": "06998806167954433550"
     },
     "user_tz": 420
    },
    "id": "pztCrQ50TfNB",
    "outputId": "8024fd9c-fbaa-4384-8d8b-e88aa382e6f1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length of the data :  90447\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f624a59410bd43d2aa8d58080e2b152e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "12 length= 73\n",
      "/home/tharun/Downloads/Gouthami/NLP_Project/Training_Data/Submission/parsed_df/parsed_data_10_12.csv generated\n"
     ]
    }
   ],
   "source": [
    "generate_files(JSON_FILE, FILE_PREFIX, start,end, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tharun/Downloads/Gouthami/NLP_Project/Training_Data/Submission/hotpot_train_v1.1.json\n"
     ]
    }
   ],
   "source": [
    "print(JSON_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "executionInfo": {
     "elapsed": 28793,
     "status": "ok",
     "timestamp": 1603574380116,
     "user": {
      "displayName": "Gouthami Kanchukatla",
      "photoUrl": "",
      "userId": "06998806167954433550"
     },
     "user_tz": 420
    },
    "id": "aO8gSTPAPiok"
   },
   "outputs": [],
   "source": [
    "from semantic_text_similarity.models import WebBertSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 581,
     "status": "ok",
     "timestamp": 1603577256225,
     "user": {
      "displayName": "Gouthami Kanchukatla",
      "photoUrl": "",
      "userId": "06998806167954433550"
     },
     "user_tz": 420
    },
    "id": "vN7OLE1K-vl-",
    "outputId": "ac0a36b9-eb15-4056-9cdf-a20ca43a658b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cur_file_name = \"\".join((FILE_PREFIX, str(start),\"_\",str(end),\".csv\"))\n",
    "new_df = pd.read_csv(cur_file_name)\n",
    "\n",
    "final_df = pd.DataFrame(columns=['question-id', 'passage-id','passage_title','passage_len','sentence_1','sentence_2','score'])\n",
    "final_df['question-id'] = new_df['question-id']\n",
    "final_df['passage-id'] =  new_df['passage-id']\n",
    "final_df['passage_title'] = new_df['passage_title']\n",
    "final_df['passage_len'] = new_df['passage_len']\n",
    "final_df['sentence_1'] = new_df['query']\n",
    "final_df['sentence_2'] =  new_df['sentence']\n",
    "print(len(final_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scores Generation section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "id": "sQq-TPZ9Bnkv",
    "outputId": "b7e98834-e800-40c2-b1a4-a7ed553c3eca",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/24/2020 23:10:51 - INFO - pytorch_transformers.tokenization_utils -   Model name '/home/tharun/.cache/torch/semantic_text_similarity/448b457fca135ab38bb3d6af49c5f220a8167e6b6ce9012f7df8172aa29865f9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '/home/tharun/.cache/torch/semantic_text_similarity/448b457fca135ab38bb3d6af49c5f220a8167e6b6ce9012f7df8172aa29865f9' is a path or url to a directory containing tokenizer files.\n",
      "10/24/2020 23:10:51 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file /home/tharun/.cache/torch/semantic_text_similarity/448b457fca135ab38bb3d6af49c5f220a8167e6b6ce9012f7df8172aa29865f9/added_tokens.json. We won't load it.\n",
      "10/24/2020 23:10:51 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file /home/tharun/.cache/torch/semantic_text_similarity/448b457fca135ab38bb3d6af49c5f220a8167e6b6ce9012f7df8172aa29865f9/special_tokens_map.json. We won't load it.\n",
      "10/24/2020 23:10:51 - INFO - pytorch_transformers.tokenization_utils -   loading file /home/tharun/.cache/torch/semantic_text_similarity/448b457fca135ab38bb3d6af49c5f220a8167e6b6ce9012f7df8172aa29865f9/vocab.txt\n",
      "10/24/2020 23:10:51 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "10/24/2020 23:10:51 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "10/24/2020 23:10:51 - INFO - pytorch_transformers.modeling_utils -   loading weights file /home/tharun/.cache/torch/semantic_text_similarity/448b457fca135ab38bb3d6af49c5f220a8167e6b6ce9012f7df8172aa29865f9/pytorch_model.bin\n",
      "10/24/2020 23:10:53 - INFO - root -   Initialized BertSentencePairSimilarity model from /home/tharun/.cache/torch/semantic_text_similarity/448b457fca135ab38bb3d6af49c5f220a8167e6b6ce9012f7df8172aa29865f9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d9ec28b90144d108bc07d73f79a5350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=73.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-72-5ef380ad6b60>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_df['score'][i]= np.round(sts_score,2)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "web_model = WebBertSimilarity(device='cuda', batch_size=16) #defaults to GPU prediction\n",
    "\n",
    "for i in tnrange(len(final_df)):\n",
    "    sts_score  = web_model.predict([(final_df['sentence_1'][i],final_df['sentence_2'][i])])\n",
    "    final_df['score'][i]= np.round(sts_score,2)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "Vn9sXuVsg3_J"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question-id</th>\n",
       "      <th>passage-id</th>\n",
       "      <th>passage_title</th>\n",
       "      <th>passage_len</th>\n",
       "      <th>sentence_1</th>\n",
       "      <th>sentence_2</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>Lights Out Paris</td>\n",
       "      <td>3</td>\n",
       "      <td>Fast Cars, Danger, Fire and Knives includes gu...</td>\n",
       "      <td>Lights Out Paris is the first studio album by ...</td>\n",
       "      <td>1.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>Lights Out Paris</td>\n",
       "      <td>3</td>\n",
       "      <td>Fast Cars, Danger, Fire and Knives includes gu...</td>\n",
       "      <td>It was released July 28, 2005 on Doomtree Rec...</td>\n",
       "      <td>1.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>Lights Out Paris</td>\n",
       "      <td>3</td>\n",
       "      <td>Fast Cars, Danger, Fire and Knives includes gu...</td>\n",
       "      <td>The album was re-released with four remixes a...</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>El-P</td>\n",
       "      <td>2</td>\n",
       "      <td>Fast Cars, Danger, Fire and Knives includes gu...</td>\n",
       "      <td>Jaime Meline (born March 2, 1975), better know...</td>\n",
       "      <td>2.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>El-P</td>\n",
       "      <td>2</td>\n",
       "      <td>Fast Cars, Danger, Fire and Knives includes gu...</td>\n",
       "      <td>Originally a member of Company Flow, El-P has...</td>\n",
       "      <td>1.45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question-id  passage-id     passage_title  passage_len  \\\n",
       "0           11           1  Lights Out Paris            3   \n",
       "1           11           1  Lights Out Paris            3   \n",
       "2           11           1  Lights Out Paris            3   \n",
       "3           11           2              El-P            2   \n",
       "4           11           2              El-P            2   \n",
       "\n",
       "                                          sentence_1  \\\n",
       "0  Fast Cars, Danger, Fire and Knives includes gu...   \n",
       "1  Fast Cars, Danger, Fire and Knives includes gu...   \n",
       "2  Fast Cars, Danger, Fire and Knives includes gu...   \n",
       "3  Fast Cars, Danger, Fire and Knives includes gu...   \n",
       "4  Fast Cars, Danger, Fire and Knives includes gu...   \n",
       "\n",
       "                                          sentence_2 score  \n",
       "0  Lights Out Paris is the first studio album by ...  1.16  \n",
       "1   It was released July 28, 2005 on Doomtree Rec...  1.35  \n",
       "2   The album was re-released with four remixes a...  0.98  \n",
       "3  Jaime Meline (born March 2, 1975), better know...  2.42  \n",
       "4   Originally a member of Company Flow, El-P has...  1.45  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "executionInfo": {
     "elapsed": 418,
     "status": "ok",
     "timestamp": 1603576660387,
     "user": {
      "displayName": "Gouthami Kanchukatla",
      "photoUrl": "",
      "userId": "06998806167954433550"
     },
     "user_tz": 420
    },
    "id": "cuggQgoxRs-J"
   },
   "outputs": [],
   "source": [
    "      \n",
    "SCORED_FOLDER = \"\".join((FOLDER, \"/scored_df/\"))\n",
    "\n",
    "filename = SCORED_FOLDER +\"with_scores_\"+ str(start) +\"_\"+str(end) +\".csv\"\n",
    "\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "final_df.to_csv(filename,index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adiLFxLMcv81"
   },
   "source": [
    "Data Frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 400,
     "status": "ok",
     "timestamp": 1603576669325,
     "user": {
      "displayName": "Gouthami Kanchukatla",
      "photoUrl": "",
      "userId": "06998806167954433550"
     },
     "user_tz": 420
    },
    "id": "zx29Srr_nIK4",
    "outputId": "88f66e2d-2109-4ff9-96ed-7ebfbb13269d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tharun/Downloads/Gouthami/NLP_Project/Training_Data/Submission/scored_df/with_scores_10_12.csv\n"
     ]
    }
   ],
   "source": [
    "print(filename)\n",
    "result_df= pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "5Dzd-O1-UKTo"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question-id</th>\n",
       "      <th>passage-id</th>\n",
       "      <th>passage_title</th>\n",
       "      <th>passage_len</th>\n",
       "      <th>sentence_1</th>\n",
       "      <th>sentence_2</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>Lights Out Paris</td>\n",
       "      <td>3</td>\n",
       "      <td>Fast Cars, Danger, Fire and Knives includes gu...</td>\n",
       "      <td>Lights Out Paris is the first studio album by ...</td>\n",
       "      <td>1.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>Lights Out Paris</td>\n",
       "      <td>3</td>\n",
       "      <td>Fast Cars, Danger, Fire and Knives includes gu...</td>\n",
       "      <td>It was released July 28, 2005 on Doomtree Rec...</td>\n",
       "      <td>1.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>Lights Out Paris</td>\n",
       "      <td>3</td>\n",
       "      <td>Fast Cars, Danger, Fire and Knives includes gu...</td>\n",
       "      <td>The album was re-released with four remixes a...</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>El-P</td>\n",
       "      <td>2</td>\n",
       "      <td>Fast Cars, Danger, Fire and Knives includes gu...</td>\n",
       "      <td>Jaime Meline (born March 2, 1975), better know...</td>\n",
       "      <td>2.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>El-P</td>\n",
       "      <td>2</td>\n",
       "      <td>Fast Cars, Danger, Fire and Knives includes gu...</td>\n",
       "      <td>Originally a member of Company Flow, El-P has...</td>\n",
       "      <td>1.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>Bruce Harwood</td>\n",
       "      <td>5</td>\n",
       "      <td>Gunmen from Laredo starred which narrator of \"...</td>\n",
       "      <td>Bruce Harwood (born April 29, 1963) is a Canad...</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>Bruce Harwood</td>\n",
       "      <td>5</td>\n",
       "      <td>Gunmen from Laredo starred which narrator of \"...</td>\n",
       "      <td>In addition to \"The X-Files\", Harwood portray...</td>\n",
       "      <td>1.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>Bruce Harwood</td>\n",
       "      <td>5</td>\n",
       "      <td>Gunmen from Laredo starred which narrator of \"...</td>\n",
       "      <td>He has also played other roles with a strong ...</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>Bruce Harwood</td>\n",
       "      <td>5</td>\n",
       "      <td>Gunmen from Laredo starred which narrator of \"...</td>\n",
       "      <td>He was a founding member of the Vancouver sum...</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>Bruce Harwood</td>\n",
       "      <td>5</td>\n",
       "      <td>Gunmen from Laredo starred which narrator of \"...</td>\n",
       "      <td>He also starred in the 1988 movie \"Earth Star...</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    question-id  passage-id     passage_title  passage_len  \\\n",
       "0            11           1  Lights Out Paris            3   \n",
       "1            11           1  Lights Out Paris            3   \n",
       "2            11           1  Lights Out Paris            3   \n",
       "3            11           2              El-P            2   \n",
       "4            11           2              El-P            2   \n",
       "..          ...         ...               ...          ...   \n",
       "68           12          10     Bruce Harwood            5   \n",
       "69           12          10     Bruce Harwood            5   \n",
       "70           12          10     Bruce Harwood            5   \n",
       "71           12          10     Bruce Harwood            5   \n",
       "72           12          10     Bruce Harwood            5   \n",
       "\n",
       "                                           sentence_1  \\\n",
       "0   Fast Cars, Danger, Fire and Knives includes gu...   \n",
       "1   Fast Cars, Danger, Fire and Knives includes gu...   \n",
       "2   Fast Cars, Danger, Fire and Knives includes gu...   \n",
       "3   Fast Cars, Danger, Fire and Knives includes gu...   \n",
       "4   Fast Cars, Danger, Fire and Knives includes gu...   \n",
       "..                                                ...   \n",
       "68  Gunmen from Laredo starred which narrator of \"...   \n",
       "69  Gunmen from Laredo starred which narrator of \"...   \n",
       "70  Gunmen from Laredo starred which narrator of \"...   \n",
       "71  Gunmen from Laredo starred which narrator of \"...   \n",
       "72  Gunmen from Laredo starred which narrator of \"...   \n",
       "\n",
       "                                           sentence_2  score  \n",
       "0   Lights Out Paris is the first studio album by ...   1.16  \n",
       "1    It was released July 28, 2005 on Doomtree Rec...   1.35  \n",
       "2    The album was re-released with four remixes a...   0.98  \n",
       "3   Jaime Meline (born March 2, 1975), better know...   2.42  \n",
       "4    Originally a member of Company Flow, El-P has...   1.45  \n",
       "..                                                ...    ...  \n",
       "68  Bruce Harwood (born April 29, 1963) is a Canad...   0.90  \n",
       "69   In addition to \"The X-Files\", Harwood portray...   1.28  \n",
       "70   He has also played other roles with a strong ...   0.71  \n",
       "71   He was a founding member of the Vancouver sum...   0.38  \n",
       "72   He also starred in the 1988 movie \"Earth Star...   0.65  \n",
       "\n",
       "[73 rows x 7 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "executionInfo": {
     "elapsed": 2851,
     "status": "ok",
     "timestamp": 1603576685036,
     "user": {
      "displayName": "Gouthami Kanchukatla",
      "photoUrl": "",
      "userId": "06998806167954433550"
     },
     "user_tz": 420
    },
    "id": "Zwg7nfDftoY9"
   },
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame(columns=['question', 'passage_title','sia_score'])\n",
    "i =  0\n",
    "while i < len(result_df):\n",
    "  p_len = result_df['passage_len'][i]\n",
    "  score_sum = 0\n",
    "  k  = 0\n",
    "  while k < p_len:\n",
    "    score_sum = score_sum +  result_df['score'][i]\n",
    "    k = k+1\n",
    "    i  = i+1\n",
    "  avg_score =  (score_sum/ p_len)\n",
    "  #print(avg_score)\n",
    "  new_row = {'question': result_df['sentence_1'][i-1], 'passage_title' : result_df['passage_title'][i-1], 'sia_score' : avg_score }\n",
    "  output_df =  output_df.append(new_row,ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "executionInfo": {
     "elapsed": 358,
     "status": "ok",
     "timestamp": 1603576701178,
     "user": {
      "displayName": "Gouthami Kanchukatla",
      "photoUrl": "",
      "userId": "06998806167954433550"
     },
     "user_tz": 420
    },
    "id": "aa__zBjInopI",
    "outputId": "b80be8c6-405a-4a41-b18e-e3adbe95c97d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>passage_title</th>\n",
       "      <th>sia_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fast Cars, Danger, Fire and Knives includes gu...</td>\n",
       "      <td>Lights Out Paris</td>\n",
       "      <td>1.163333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fast Cars, Danger, Fire and Knives includes gu...</td>\n",
       "      <td>El-P</td>\n",
       "      <td>1.935000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fast Cars, Danger, Fire and Knives includes gu...</td>\n",
       "      <td>Born and Raised (EP)</td>\n",
       "      <td>1.267500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fast Cars, Danger, Fire and Knives includes gu...</td>\n",
       "      <td>Lord Steppington</td>\n",
       "      <td>1.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fast Cars, Danger, Fire and Knives includes gu...</td>\n",
       "      <td>Control Freek</td>\n",
       "      <td>1.235000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fast Cars, Danger, Fire and Knives includes gu...</td>\n",
       "      <td>Hip hop</td>\n",
       "      <td>1.156000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Fast Cars, Danger, Fire and Knives includes gu...</td>\n",
       "      <td>Fast Cars, Danger, Fire and Knives</td>\n",
       "      <td>1.565000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Fast Cars, Danger, Fire and Knives includes gu...</td>\n",
       "      <td>Longterm Mentality</td>\n",
       "      <td>1.272500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Fast Cars, Danger, Fire and Knives includes gu...</td>\n",
       "      <td>Experimental hip hop</td>\n",
       "      <td>1.403333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Fast Cars, Danger, Fire and Knives includes gu...</td>\n",
       "      <td>Changes (Alyson Avenue album)</td>\n",
       "      <td>1.123333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Gunmen from Laredo starred which narrator of \"...</td>\n",
       "      <td>Louis Stevens (writer)</td>\n",
       "      <td>0.832857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Gunmen from Laredo starred which narrator of \"...</td>\n",
       "      <td>The Lone Gunmen</td>\n",
       "      <td>1.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Gunmen from Laredo starred which narrator of \"...</td>\n",
       "      <td>Hermanos de Pistoleros Latinos</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Gunmen from Laredo starred which narrator of \"...</td>\n",
       "      <td>Piranha (1972 film)</td>\n",
       "      <td>1.610000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Gunmen from Laredo starred which narrator of \"...</td>\n",
       "      <td>Gunmen from Laredo</td>\n",
       "      <td>2.460000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Gunmen from Laredo starred which narrator of \"...</td>\n",
       "      <td>Seemanto-heera</td>\n",
       "      <td>0.555000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Gunmen from Laredo starred which narrator of \"...</td>\n",
       "      <td>Frontier Texas!</td>\n",
       "      <td>1.092857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Gunmen from Laredo starred which narrator of \"...</td>\n",
       "      <td>Walter Coy</td>\n",
       "      <td>1.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Gunmen from Laredo starred which narrator of \"...</td>\n",
       "      <td>Max McLean</td>\n",
       "      <td>0.568000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Gunmen from Laredo starred which narrator of \"...</td>\n",
       "      <td>Bruce Harwood</td>\n",
       "      <td>0.784000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0   Fast Cars, Danger, Fire and Knives includes gu...   \n",
       "1   Fast Cars, Danger, Fire and Knives includes gu...   \n",
       "2   Fast Cars, Danger, Fire and Knives includes gu...   \n",
       "3   Fast Cars, Danger, Fire and Knives includes gu...   \n",
       "4   Fast Cars, Danger, Fire and Knives includes gu...   \n",
       "5   Fast Cars, Danger, Fire and Knives includes gu...   \n",
       "6   Fast Cars, Danger, Fire and Knives includes gu...   \n",
       "7   Fast Cars, Danger, Fire and Knives includes gu...   \n",
       "8   Fast Cars, Danger, Fire and Knives includes gu...   \n",
       "9   Fast Cars, Danger, Fire and Knives includes gu...   \n",
       "10  Gunmen from Laredo starred which narrator of \"...   \n",
       "11  Gunmen from Laredo starred which narrator of \"...   \n",
       "12  Gunmen from Laredo starred which narrator of \"...   \n",
       "13  Gunmen from Laredo starred which narrator of \"...   \n",
       "14  Gunmen from Laredo starred which narrator of \"...   \n",
       "15  Gunmen from Laredo starred which narrator of \"...   \n",
       "16  Gunmen from Laredo starred which narrator of \"...   \n",
       "17  Gunmen from Laredo starred which narrator of \"...   \n",
       "18  Gunmen from Laredo starred which narrator of \"...   \n",
       "19  Gunmen from Laredo starred which narrator of \"...   \n",
       "\n",
       "                         passage_title  sia_score  \n",
       "0                     Lights Out Paris   1.163333  \n",
       "1                                 El-P   1.935000  \n",
       "2                 Born and Raised (EP)   1.267500  \n",
       "3                     Lord Steppington   1.450000  \n",
       "4                        Control Freek   1.235000  \n",
       "5                              Hip hop   1.156000  \n",
       "6   Fast Cars, Danger, Fire and Knives   1.565000  \n",
       "7                   Longterm Mentality   1.272500  \n",
       "8                 Experimental hip hop   1.403333  \n",
       "9        Changes (Alyson Avenue album)   1.123333  \n",
       "10              Louis Stevens (writer)   0.832857  \n",
       "11                     The Lone Gunmen   1.140000  \n",
       "12      Hermanos de Pistoleros Latinos   0.975000  \n",
       "13                 Piranha (1972 film)   1.610000  \n",
       "14                  Gunmen from Laredo   2.460000  \n",
       "15                      Seemanto-heera   0.555000  \n",
       "16                     Frontier Texas!   1.092857  \n",
       "17                          Walter Coy   1.730000  \n",
       "18                          Max McLean   0.568000  \n",
       "19                       Bruce Harwood   0.784000  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "executionInfo": {
     "elapsed": 396,
     "status": "ok",
     "timestamp": 1603576688612,
     "user": {
      "displayName": "Gouthami Kanchukatla",
      "photoUrl": "",
      "userId": "06998806167954433550"
     },
     "user_tz": 420
    },
    "id": "2yOn2ZJIna9g"
   },
   "outputs": [],
   "source": [
    "output_df = output_df.round({'sia_score': 2})\n",
    "\n",
    "\n",
    "OUT_FOLDER = \"\".join((FOLDER, \"/output_df/\"))\n",
    "filename = OUT_FOLDER +\"output_\"+ str(start) +\"_\"+str(end) +\".csv\"\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "output_df.to_csv(filename,index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The End #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XTFB-HW7yDXc"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5ElDQxtZco9"
   },
   "source": [
    "# Experiments on MODELs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "executionInfo": {
     "elapsed": 28774,
     "status": "aborted",
     "timestamp": 1603574380121,
     "user": {
      "displayName": "Gouthami Kanchukatla",
      "photoUrl": "",
      "userId": "06998806167954433550"
     },
     "user_tz": 420
    },
    "id": "G6fgG19HZViz"
   },
   "outputs": [],
   "source": [
    "# #Class for Regression\n",
    "# class Regressor(nn.Module):\n",
    "\n",
    "#   def __init__(self,  model_path):\n",
    "#     super(Regressor, self).__init__()\n",
    "#     # self.bert = BertModel.from_pretrained(model_path)\n",
    "#     self.bert = BertModel.from_pretrained('/content/drive/My Drive/Courses/NLP/Project/model')\n",
    "#     self.out = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "#   def forward(self, input_ids, attention_mask):\n",
    "#     output, pooler_out = self.bert(input_ids=input_ids, attention_mask=attention_mask)Preprocessing\n",
    "#     score= self.out(pooler_out)\n",
    "#     return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "executionInfo": {
     "elapsed": 28772,
     "status": "aborted",
     "timestamp": 1603574380121,
     "user": {
      "displayName": "Gouthami Kanchukatla",
      "photoUrl": "",
      "userId": "06998806167954433550"
     },
     "user_tz": 420
    },
    "id": "Z724AT3Mu7du"
   },
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# n_gpu = torch.cuda.device_count()\n",
    "# print(\"device: {} n_gpu: {}\".format(device, n_gpu)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "executionInfo": {
     "elapsed": 28771,
     "status": "aborted",
     "timestamp": 1603574380122,
     "user": {
      "displayName": "Gouthami Kanchukatla",
      "photoUrl": "",
      "userId": "06998806167954433550"
     },
     "user_tz": 420
    },
    "id": "cUKPkQelvE_1"
   },
   "outputs": [],
   "source": [
    "# logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "#                     datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "#                     level = logging.INFO)\n",
    "# logger = logging.getLogger(__name__)\n",
    "# print(logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "executionInfo": {
     "elapsed": 28768,
     "status": "aborted",
     "timestamp": 1603574380122,
     "user": {
      "displayName": "Gouthami Kanchukatla",
      "photoUrl": "",
      "userId": "06998806167954433550"
     },
     "user_tz": 420
    },
    "id": "WdiixvzzLla-"
   },
   "outputs": [],
   "source": [
    "# # memory footprint support libraries/code\n",
    "# !ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
    "# !pip install gputil\n",
    "# !pip install psutil\n",
    "# !pip install humanize\n",
    "\n",
    "# import psutil\n",
    "# import humanize\n",
    "# import os\n",
    "# import GPUtil as GPU\n",
    "\n",
    "# GPUs = GPU.getGPUs()\n",
    "# # XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
    "# gpu = GPUs[0]\n",
    "# def printm():\n",
    "#     process = psutil.Process(os.getpid())\n",
    "#     print(\"Gen RAM Free: \" + humanize.naturalsize(psutil.virtual_memory().available), \" |     Proc size: \" + humanize.naturalsize(process.memory_info().rss))\n",
    "#     print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total     {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
    "# printm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9YAfRS5aTXB"
   },
   "source": [
    "# TRAINING AND EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "executionInfo": {
     "elapsed": 28768,
     "status": "aborted",
     "timestamp": 1603574380124,
     "user": {
      "displayName": "Gouthami Kanchukatla",
      "photoUrl": "",
      "userId": "06998806167954433550"
     },
     "user_tz": 420
    },
    "id": "TLOvLUp5aWyv"
   },
   "outputs": [],
   "source": [
    "# output_dir= '/content/drive/My Drive/Courses/NLP/Project/model'\n",
    "# output_result= '/content/drive/My Drive/Courses/NLP/Project/results'\n",
    "\n",
    "# if not os.path.exists(output_dir):\n",
    "#   os.makedirs(output_dir)\n",
    "\n",
    "# if not os.path.exists(output_result):\n",
    "#   os.makedirs(output_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "executionInfo": {
     "elapsed": 28766,
     "status": "aborted",
     "timestamp": 1603574380124,
     "user": {
      "displayName": "Gouthami Kanchukatla",
      "photoUrl": "",
      "userId": "06998806167954433550"
     },
     "user_tz": 420
    },
    "id": "M1ZM4N0abpyM"
   },
   "outputs": [],
   "source": [
    "# for iteration in tnrange(epochs, desc='Epochs'):\n",
    "#   model.train()\n",
    "#   logger.info(\"Running for iteration: {}\".format(iteration+1))\n",
    "\n",
    "#   training_loss, training_steps=0,0\n",
    "#   true_labels, predicted_labels= list(), list()\n",
    "  \n",
    "#   for step, batch in enumerate(train_dataloader):\n",
    "#     batch = tuple(t.to(device) for t in batch)\n",
    "#     ip_ids, masks, gold_labels= batch\n",
    "#     score = model(ip_ids, attention_mask=masks)\n",
    "#     score = score.squeeze(1)\n",
    "#     loss= mse_loss(score, gold_labels.float())\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     optimizer.zero_grad()\n",
    "#     training_loss+=loss.item()\n",
    "#     training_steps+=1\n",
    "#     if (step+1)%1000 == 0:\n",
    "#       print(step+1)\n",
    "\n",
    "#     true_labels.extend(gold_labels.cpu().numpy())\n",
    "#     predicted_labels.extend(score.detach().cpu().numpy())\n",
    "  \n",
    "#   training_loss_for_epoch= training_loss/training_steps\n",
    "#   pcc= pearsonr(true_labels, predicted_labels)\n",
    "#   result = {'loss': training_loss_for_epoch, 'PCC': pcc[0]}\n",
    "#   print(result)\n",
    "\n",
    "#   model_to_save = model.bert.module if hasattr(model.bert, 'module') else model.bert\n",
    "#   model_to_save.save_pretrained(output_dir)\n",
    "\n",
    "#   torch.save(model.out.state_dict(), join(output_dir, 'model_state.bin'))\n",
    "\n",
    "#   print(\"Running validation for epoch: {}\".format(iteration+1))\n",
    "\n",
    "#   validation_loss, validation_steps=0,0\n",
    "#   true_labels, predicted_labels= list(), list()\n",
    "\n",
    "#   for step, batch in enumerate(dev_dataloader):\n",
    "#     batch = tuple(t.to(device) for t in batch)\n",
    "#     ip_ids, masks, gold_labels= batch\n",
    "#     score = model(ip_ids, attention_mask=masks)\n",
    "#     score = score.squeeze(1)\n",
    "#     loss= mse_loss(score, gold_labels)\n",
    "#     validation_loss+=loss.item()\n",
    "#     validation_steps+=1\n",
    "\n",
    "#     true_labels.extend(gold_labels.cpu().numpy())\n",
    "#     predicted_labels.extend(score.detach().cpu().numpy())\n",
    "  \n",
    "#   val_loss_for_epoch= validation_loss/validation_steps\n",
    "#   pcc= pearsonr(true_labels, predicted_labels)\n",
    "#   result = {'loss':val_loss_for_epoch, 'PCC': pcc[0]}\n",
    "#   print(result)\n",
    "  \n",
    "#   #Testing\n",
    "\n",
    "#   print(\"Running evaluation for epoch: {}\".format(iteration+1))\n",
    "\n",
    "#   true_labels, predicted_labels= list(), list()Preprocessing\n",
    "#   model.eval()\n",
    "#   with torch.no_grad():\n",
    "#     for step, batch in enumerate(test_dataloader):\n",
    "#       batch = tuple(t.to(device) for t in batch)\n",
    "#       ip_ids, masks, gold_labels= batch\n",
    "#       score = model(ip_ids, attention_mask=masks)\n",
    "#       score = score.squeeze(1)\n",
    "\n",
    "#       true_labels.extend(gold_labels.cpu().numpy())\n",
    "#       predicted_labels.extend(score.detach().cpu().numpy())\n",
    "  \n",
    "#   pcc= pearsonr(true_labels, predicted_labels)\n",
    "#   test_report= {'PCC': pcc[0]}\n",
    "#   print(test_report)\n",
    "\n",
    "#   with open(join(output_result, 'result_'+str(iteration+1)+'.json'), 'w') as fp:\n",
    "#     json.dump(test_report, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4lNTpj1dwSl"
   },
   "source": [
    "Extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "executionInfo": {
     "elapsed": 28765,
     "status": "aborted",
     "timestamp": 1603574380125,
     "user": {
      "displayName": "Gouthami Kanchukatla",
      "photoUrl": "",
      "userId": "06998806167954433550"
     },
     "user_tz": 420
    },
    "id": "jvGwjitSdog3"
   },
   "outputs": [],
   "source": [
    "# def sts_score_generator(df):\n",
    "#   tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#   #tokenizer = BertTokenizer.from_pretrained('/content/drive/My Drive/Courses/NLP/Project/model')\n",
    "#   sts_dataloader = create_dataloader(tokenizer,df)\n",
    "#   sts_score  = []\n",
    "#   for step, batch in enumerate(sts_dataloader):\n",
    "#     batch = tuple(t.to(device) for t in batch)\n",
    "#     ip_ids,masks = batch\n",
    "#     score = model(ip_ids, attention_mask = masks)\n",
    "#     score  = score.squeeze(1)\n",
    "#     sts_score.extend(score.detach().cpu().numpy())\n",
    "#   return sts_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "executionInfo": {
     "elapsed": 28764,
     "status": "aborted",
     "timestamp": 1603574380125,
     "user": {
      "displayName": "Gouthami Kanchukatla",
      "photoUrl": "",
      "userId": "06998806167954433550"
     },
     "user_tz": 420
    },
    "id": "jZ3Pe8J5x5lR"
   },
   "outputs": [],
   "source": [
    "# def sts_score_generator(df):\n",
    "#   # tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#   sts_dataloader = create_dataloader(tokenizer,df)\n",
    "#   sts_score  = []\n",
    "#   for step, batch in enumerate(sts_dataloader):\n",
    "#     batch = tuple(t.to(device) for t in batch)\n",
    "#     ip_ids,masks = batch\n",
    "#     score = model(ip_ids, attention_mask = masks)\n",
    "#     score  = score.squeeze(1)\n",
    "#     sts_score.extend(score.detach().cpu().numpy())\n",
    "#   return sts_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "executionInfo": {
     "elapsed": 28763,
     "status": "aborted",
     "timestamp": 1603574380126,
     "user": {
      "displayName": "Gouthami Kanchukatla",
      "photoUrl": "",
      "userId": "06998806167954433550"
     },
     "user_tz": 420
    },
    "id": "fyUlXVlHPwk5"
   },
   "outputs": [],
   "source": [
    "# load_data= '/content/drive/My Drive/Courses/NLP/Project/data'\n",
    "# train_df= pd.read_csv(join(load_data,'train.csv'))\n",
    "# train_df.columns =['caption', 'MSR', 'test','id', 'label','sentence_1','sentence_2','url','url_2']\n",
    "# dev_df= pd.read_csv(join(load_data,'dev.csv'))\n",
    "# dev_df.columns =['caption', 'MSR', 'test','id', 'label','sentence_1','sentence_2','url','url_2']\n",
    "# test_df= pd.read_csv(join(load_data,'test.csv'))\n",
    "# #test_df = new_df\n",
    "# test_df.columns =['caption', 'MSR', 'test','id', 'label','sentence_1','sentence_2','url','url_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "executionInfo": {
     "elapsed": 28761,
     "status": "aborted",
     "timestamp": 1603574380126,
     "user": {
      "displayName": "Gouthami Kanchukatla",
      "photoUrl": "",
      "userId": "06998806167954433550"
     },
     "user_tz": 420
    },
    "id": "SRcCbBjwZyYr"
   },
   "outputs": [],
   "source": [
    "# #Model Intialization\n",
    "\n",
    "# #epochs=10\n",
    "\n",
    "# #Load Model\n",
    "# model_path  = '/content/drive/My Drive/Courses/NLP/Project/model'\n",
    "# model= Regressor(model_path)\n",
    "# weights_score = torch.load(join(model_path,'model_state.bin'))\n",
    "# model.out.load_state_dict(weights_score)\n",
    "# model.to(device)\n",
    "\n",
    "# #To tokenize  the data\n",
    "# #tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# # # Prepare optimizer\n",
    "# # optimizer = AdamW(model.parameters(),lr=2e-5)\n",
    "\n",
    "# # #Loss Function\n",
    "# # mse_loss= nn.MSELoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "executionInfo": {
     "elapsed": 28760,
     "status": "aborted",
     "timestamp": 1603574380127,
     "user": {
      "displayName": "Gouthami Kanchukatla",
      "photoUrl": "",
      "userId": "06998806167954433550"
     },
     "user_tz": 420
    },
    "id": "61GBl4kBRXWr"
   },
   "outputs": [],
   "source": [
    "# tokenizer = BertTokenizer.from_pretrained('/content/drive/My Drive/Courses/NLP/Project/model')\n",
    "# #tokenizer = BertTokenizer.from_pretrained('/content/drive/My Drive/Courses/NLP/Project/model')\n",
    "# # test_dataloader = create_dataloader(tokenizer, train_df)\n",
    "# # train_dataloader = create_dataloader(tokenizer, train_df)\n",
    "# # dev_dataloader = create_dataloader(tokenizer, dev_df)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN/PFRRASLIUkkJ0/Iu51H0",
   "collapsed_sections": [],
   "name": "Data_PreProcessing_NLP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
