{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data_PreProcessing_NLP_Latest.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "467526d45f5c48d6be1a549c78c72df6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_60d5b039e20c41f5a83f8ee6dbcdf9bd",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4141c4c1dd0c4db6b29e16d3908c7e24",
              "IPY_MODEL_de9eeafde7fc4eccb7d68395863334c9"
            ]
          }
        },
        "60d5b039e20c41f5a83f8ee6dbcdf9bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4141c4c1dd0c4db6b29e16d3908c7e24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_251a1314473f4ace9a68f62f375666eb",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 5,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 5,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ee5c13b3d9434d5ab7855fd47dfaa30e"
          }
        },
        "de9eeafde7fc4eccb7d68395863334c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4e2b7ec0172a4e5caf6fe15d3f53b101",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5/5 [00:00&lt;00:00,  6.58it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d037a85d11004e2997f136603161cb65"
          }
        },
        "251a1314473f4ace9a68f62f375666eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ee5c13b3d9434d5ab7855fd47dfaa30e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4e2b7ec0172a4e5caf6fe15d3f53b101": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d037a85d11004e2997f136603161cb65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFxtExkYOK4s"
      },
      "source": [
        "# Necessary Installations section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89yzY2GYTHgy"
      },
      "source": [
        "!pip3 install transformers\n",
        "!pip3 install unidecode\n",
        "!pip install pandas\n",
        "!pip install keras \n",
        "!pip install tensorflow\n",
        "!pip install torch\n",
        "!pip install git+https://github.com/AndriyMulyar/semantic-text-similarity\n",
        "!pip install urllib3==1.25.10\n",
        "!pip install awscli awsebcli botocore==1.18.18 --upgrade\n",
        "!pip install tqdm\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgJirCHsZoO4"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from os import listdir\n",
        "from os.path import join\n",
        "import unidecode\n",
        "import re\n",
        "import logging\n",
        "from tqdm.notebook import tnrange\n",
        "import glob\n",
        "import json\n",
        "import os\n",
        "\n",
        "#For ploting results\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# DL Libraries\n",
        "from transformers import BertModel, AdamW, BertTokenizer, BertConfig, get_linear_schedule_with_warmup\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,TensorDataset)\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.stats import pearsonr\n",
        "from semantic_text_similarity.models import WebBertSimilarity\n",
        "\n",
        "# #NLTK Libraries\n",
        "# import nltk\n",
        "# nltk.download('punkt')\n",
        "# from nltk.tokenize import word_tokenize\n",
        "# from nltk.corpus import stopwords\n",
        "# nltk.download('stopwords')\n",
        "# stop_words = set(stopwords.words('english'))\n",
        "# nltk.download('wordnet')\n",
        "# nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Btlpy-D9dYDL"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "# print(\"device: {} n_gpu: {}\".format(device, n_gpu))\n",
        "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
        "                    level = logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "# print(logger)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTySPEHF_T5-",
        "outputId": "ee3af8de-a760-4670-8ef7-c3eaeaa983b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYGMaJhmOK42"
      },
      "source": [
        "Sample Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCzkGq8uJC0d"
      },
      "source": [
        "from semantic_text_similarity.models import WebBertSimilarity\n",
        "\n",
        "web_model = WebBertSimilarity(device='cuda', batch_size=10) #defaults to GPU prediction\n",
        "\n",
        "#clinical_model = ClinicalBertSimilarity(device='cuda', batch_size=10) #defaults to GPU prediction\n",
        "\n",
        "web_model.predict([(\"She won an olympic gold medal\",\"The women is an olympic champion\")])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVRsIv1YGIqZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0kl-1vJRo6e"
      },
      "source": [
        "# Data Preprocessing section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kF-57dt5c_AO"
      },
      "source": [
        "Function Definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4BvDcc8T7uW"
      },
      "source": [
        "\n",
        "def generate_df(df, start,end):\n",
        "    new_df = pd.DataFrame(columns=['question-id','passage-id','query','passage','sentence', 'passage_len'])\n",
        "    pd.set_option('display.max_seq_items', None)\n",
        "    num_psges =  0\n",
        "    print(\"max length of the data : \",len(df))\n",
        "\n",
        "    for i in tnrange(start,end):\n",
        "        cntxt_len = len(df['context'][i])\n",
        "        num_psges  = num_psges + cntxt_len\n",
        "\n",
        "        query = str(df['question'][i] + df['answer'][i])\n",
        "        if len(query) < 2:\n",
        "            continue\n",
        "\n",
        "        for j  in range(0,cntxt_len):\n",
        "            passage  =  \"\".join(df['context'][i][j][1])\n",
        "            senList  = df['context'][i][j][1]\n",
        "            # passage_title =  df['context'][i][j][0]\n",
        "            for each_sen in senList:\n",
        "                # filtering empty sentences\n",
        "                if len(each_sen) < 2 :\n",
        "                    continue\n",
        "\n",
        "                ques_id = str(i+1)\n",
        "                psge_id  =  str(j+1)\n",
        "                new_row = {'question-id':ques_id,'passage-id':psge_id, 'query':query,'passage': passage,'sentence': each_sen,'passage_len': len(senList)}\n",
        "                \n",
        "                new_df =  new_df.append(new_row,ignore_index= True)\n",
        "    return  new_df"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CUqXgCZUGDQ"
      },
      "source": [
        "def generate_files(inpfilename,  start,  end):\n",
        "    df = pd.read_json(inpfilename)\n",
        "    new_df  =  generate_df(df,start,end)\n",
        "    final_df = pd.DataFrame(columns=['question-id', 'passage-id','passage','passage_len','sentence_1','sentence_2','score'])\n",
        "    final_df['question-id'] = new_df['question-id']\n",
        "    final_df['passage-id'] =  new_df['passage-id']\n",
        "    final_df['passage'] = new_df['passage']\n",
        "    final_df['passage_len'] = new_df['passage_len']\n",
        "    final_df['sentence_1'] = new_df['query']\n",
        "    final_df['sentence_2'] =  new_df['sentence']\n",
        "    return final_df\n",
        "    #new_df  =  generate_df(df,start,end)\n",
        "    # str1 = foldername \n",
        "    # str3 = \".csv\"\n",
        "    # fullfilename = \"\".join((str1,str(start), \"_\", str(end),str3))\n",
        "    \n",
        "    # os.makedirs(os.path.dirname(fullfilename), exist_ok=True)\n",
        "\n",
        "    # if os.path.isfile(fullfilename) and forcegenerate == False:\n",
        "    #     print(fullfilename + \" already  created -  so skipping  generation\")\n",
        "    # else:\n",
        "    #     new_df.to_csv(fullfilename,index = False)\n",
        "    #     print(fullfilename + \" generated\")\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqID-4q3dBWI"
      },
      "source": [
        "Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqmICuDnYpsA"
      },
      "source": [
        "\n",
        "start = 0\n",
        "end = 5\n",
        "FOLDER = '/content/drive/My Drive/Courses/NLP/Project/data'\n",
        "JSON_FILE =  FOLDER +\"/hotpot_train_v1.1.json\"\n",
        "# FILE_PREFIX  =  FOLDER +  '/parsed_df/parsed_data_'"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wR5_aVgwPneH",
        "outputId": "09a6c086-39c0-4b7b-c078-a59ff2d3ac45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "JSON_FILE"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/Courses/NLP/Project/data/hotpot_train_v1.1.json'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pztCrQ50TfNB",
        "scrolled": true,
        "outputId": "a210c0b6-6b10-462d-dffc-b4e1f2002da7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100,
          "referenced_widgets": [
            "467526d45f5c48d6be1a549c78c72df6",
            "60d5b039e20c41f5a83f8ee6dbcdf9bd",
            "4141c4c1dd0c4db6b29e16d3908c7e24",
            "de9eeafde7fc4eccb7d68395863334c9",
            "251a1314473f4ace9a68f62f375666eb",
            "ee5c13b3d9434d5ab7855fd47dfaa30e",
            "4e2b7ec0172a4e5caf6fe15d3f53b101",
            "d037a85d11004e2997f136603161cb65"
          ]
        }
      },
      "source": [
        "# generate_files(JSON_FILE, FILE_PREFIX, start,end, True)\n",
        "final_df =  generate_files(JSON_FILE,start, end)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10/30/2020 06:12:24 - INFO - numexpr.utils -   NumExpr defaulting to 2 threads.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "max length of the data :  90447\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "467526d45f5c48d6be1a549c78c72df6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZ5DsBO5OK5N"
      },
      "source": [
        "# Scores Generation section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQq-TPZ9Bnkv",
        "scrolled": false
      },
      "source": [
        "web_model = WebBertSimilarity(device='cuda', batch_size=16) #defaults to GPU prediction\n",
        "\n",
        "for i in tnrange(len(final_df)):\n",
        "    sts_score  = web_model.predict([(final_df['sentence_1'][i],final_df['sentence_2'][i])])\n",
        "    final_df['score'][i]= np.round(sts_score,2)[0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ueb81UTOK5c"
      },
      "source": [
        "# Data Processing Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zwg7nfDftoY9"
      },
      "source": [
        "output_df = pd.DataFrame(columns=['question', 'sentence','sia_score'])\n",
        "i =  0\n",
        "while i < len(final_df):\n",
        "  p_len = final_df['passage_len'][i]\n",
        "  score_sum = 0\n",
        "  k  = 0\n",
        "  while k < p_len:\n",
        "    score_sum = score_sum +  final_df['score'][i]\n",
        "    k = k+1\n",
        "    i  = i+1\n",
        "  avg_score =  (score_sum/ p_len)\n",
        "  new_row = {'question': final_df['sentence_1'][i-1], 'sentence' : final_df['passage'][i-1], 'sia_score' : avg_score }\n",
        "  output_df =  output_df.append(new_row,ignore_index= True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yOn2ZJIna9g"
      },
      "source": [
        "output_df = output_df.round({'sia_score': 2})\n",
        "OUT_FOLDER = \"\".join((FOLDER, \"/output_df/\"))\n",
        "filename = OUT_FOLDER +\"output_with_passage_\"+ str(start) +\"_\"+str(end) +\".csv\"\n",
        "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "output_df.to_csv(filename,index = False)\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvcJ73MiJK_c"
      },
      "source": [
        "output_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UInwoCV2OK5i"
      },
      "source": [
        "# The End #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTFB-HW7yDXc"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5ElDQxtZco9"
      },
      "source": [
        "# Experiments on MODELs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6fgG19HZViz"
      },
      "source": [
        "# #Class for Regression\n",
        "# class Regressor(nn.Module):\n",
        "\n",
        "#   def __init__(self,  model_path):\n",
        "#     super(Regressor, self).__init__()\n",
        "#     # self.bert = BertModel.from_pretrained(model_path)\n",
        "#     self.bert = BertModel.from_pretrained('/content/drive/My Drive/Courses/NLP/Project/model')\n",
        "#     self.out = nn.Linear(self.bert.config.hidden_size, 1)\n",
        "\n",
        "#   def forward(self, input_ids, attention_mask):\n",
        "#     output, pooler_out = self.bert(input_ids=input_ids, attention_mask=attention_mask)Preprocessing\n",
        "#     score= self.out(pooler_out)\n",
        "#     return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z724AT3Mu7du"
      },
      "source": [
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# n_gpu = torch.cuda.device_count()\n",
        "# print(\"device: {} n_gpu: {}\".format(device, n_gpu)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUKPkQelvE_1"
      },
      "source": [
        "# logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "#                     datefmt = '%m/%d/%Y %H:%M:%S',\n",
        "#                     level = logging.INFO)\n",
        "# logger = logging.getLogger(__name__)\n",
        "# print(logger)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdiixvzzLla-"
      },
      "source": [
        "# # memory footprint support libraries/code\n",
        "# !ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "# !pip install gputil\n",
        "# !pip install psutil\n",
        "# !pip install humanize\n",
        "\n",
        "# import psutil\n",
        "# import humanize\n",
        "# import os\n",
        "# import GPUtil as GPU\n",
        "\n",
        "# GPUs = GPU.getGPUs()\n",
        "# # XXX: only one GPU on Colab and isn’t guaranteed\n",
        "# gpu = GPUs[0]\n",
        "# def printm():\n",
        "#     process = psutil.Process(os.getpid())\n",
        "#     print(\"Gen RAM Free: \" + humanize.naturalsize(psutil.virtual_memory().available), \" |     Proc size: \" + humanize.naturalsize(process.memory_info().rss))\n",
        "#     print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total     {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "# printm()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9YAfRS5aTXB"
      },
      "source": [
        "# TRAINING AND EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLOvLUp5aWyv"
      },
      "source": [
        "# output_dir= '/content/drive/My Drive/Courses/NLP/Project/model'\n",
        "# output_result= '/content/drive/My Drive/Courses/NLP/Project/results'\n",
        "\n",
        "# if not os.path.exists(output_dir):\n",
        "#   os.makedirs(output_dir)\n",
        "\n",
        "# if not os.path.exists(output_result):\n",
        "#   os.makedirs(output_result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1ZM4N0abpyM"
      },
      "source": [
        "# for iteration in tnrange(epochs, desc='Epochs'):\n",
        "#   model.train()\n",
        "#   logger.info(\"Running for iteration: {}\".format(iteration+1))\n",
        "\n",
        "#   training_loss, training_steps=0,0\n",
        "#   true_labels, predicted_labels= list(), list()\n",
        "  \n",
        "#   for step, batch in enumerate(train_dataloader):\n",
        "#     batch = tuple(t.to(device) for t in batch)\n",
        "#     ip_ids, masks, gold_labels= batch\n",
        "#     score = model(ip_ids, attention_mask=masks)\n",
        "#     score = score.squeeze(1)\n",
        "#     loss= mse_loss(score, gold_labels.float())\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "#     optimizer.zero_grad()\n",
        "#     training_loss+=loss.item()\n",
        "#     training_steps+=1\n",
        "#     if (step+1)%1000 == 0:\n",
        "#       print(step+1)\n",
        "\n",
        "#     true_labels.extend(gold_labels.cpu().numpy())\n",
        "#     predicted_labels.extend(score.detach().cpu().numpy())\n",
        "  \n",
        "#   training_loss_for_epoch= training_loss/training_steps\n",
        "#   pcc= pearsonr(true_labels, predicted_labels)\n",
        "#   result = {'loss': training_loss_for_epoch, 'PCC': pcc[0]}\n",
        "#   print(result)\n",
        "\n",
        "#   model_to_save = model.bert.module if hasattr(model.bert, 'module') else model.bert\n",
        "#   model_to_save.save_pretrained(output_dir)\n",
        "\n",
        "#   torch.save(model.out.state_dict(), join(output_dir, 'model_state.bin'))\n",
        "\n",
        "#   print(\"Running validation for epoch: {}\".format(iteration+1))\n",
        "\n",
        "#   validation_loss, validation_steps=0,0\n",
        "#   true_labels, predicted_labels= list(), list()\n",
        "\n",
        "#   for step, batch in enumerate(dev_dataloader):\n",
        "#     batch = tuple(t.to(device) for t in batch)\n",
        "#     ip_ids, masks, gold_labels= batch\n",
        "#     score = model(ip_ids, attention_mask=masks)\n",
        "#     score = score.squeeze(1)\n",
        "#     loss= mse_loss(score, gold_labels)\n",
        "#     validation_loss+=loss.item()\n",
        "#     validation_steps+=1\n",
        "\n",
        "#     true_labels.extend(gold_labels.cpu().numpy())\n",
        "#     predicted_labels.extend(score.detach().cpu().numpy())\n",
        "  \n",
        "#   val_loss_for_epoch= validation_loss/validation_steps\n",
        "#   pcc= pearsonr(true_labels, predicted_labels)\n",
        "#   result = {'loss':val_loss_for_epoch, 'PCC': pcc[0]}\n",
        "#   print(result)\n",
        "  \n",
        "#   #Testing\n",
        "\n",
        "#   print(\"Running evaluation for epoch: {}\".format(iteration+1))\n",
        "\n",
        "#   true_labels, predicted_labels= list(), list()Preprocessing\n",
        "#   model.eval()\n",
        "#   with torch.no_grad():\n",
        "#     for step, batch in enumerate(test_dataloader):\n",
        "#       batch = tuple(t.to(device) for t in batch)\n",
        "#       ip_ids, masks, gold_labels= batch\n",
        "#       score = model(ip_ids, attention_mask=masks)\n",
        "#       score = score.squeeze(1)\n",
        "\n",
        "#       true_labels.extend(gold_labels.cpu().numpy())\n",
        "#       predicted_labels.extend(score.detach().cpu().numpy())\n",
        "  \n",
        "#   pcc= pearsonr(true_labels, predicted_labels)\n",
        "#   test_report= {'PCC': pcc[0]}\n",
        "#   print(test_report)\n",
        "\n",
        "#   with open(join(output_result, 'result_'+str(iteration+1)+'.json'), 'w') as fp:\n",
        "#     json.dump(test_report, fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4lNTpj1dwSl"
      },
      "source": [
        "Extras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvGwjitSdog3"
      },
      "source": [
        "# def sts_score_generator(df):\n",
        "#   tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "#   #tokenizer = BertTokenizer.from_pretrained('/content/drive/My Drive/Courses/NLP/Project/model')\n",
        "#   sts_dataloader = create_dataloader(tokenizer,df)\n",
        "#   sts_score  = []\n",
        "#   for step, batch in enumerate(sts_dataloader):\n",
        "#     batch = tuple(t.to(device) for t in batch)\n",
        "#     ip_ids,masks = batch\n",
        "#     score = model(ip_ids, attention_mask = masks)\n",
        "#     score  = score.squeeze(1)\n",
        "#     sts_score.extend(score.detach().cpu().numpy())\n",
        "#   return sts_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZ3Pe8J5x5lR"
      },
      "source": [
        "# def sts_score_generator(df):\n",
        "#   # tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "#   sts_dataloader = create_dataloader(tokenizer,df)\n",
        "#   sts_score  = []\n",
        "#   for step, batch in enumerate(sts_dataloader):\n",
        "#     batch = tuple(t.to(device) for t in batch)\n",
        "#     ip_ids,masks = batch\n",
        "#     score = model(ip_ids, attention_mask = masks)\n",
        "#     score  = score.squeeze(1)\n",
        "#     sts_score.extend(score.detach().cpu().numpy())\n",
        "#   return sts_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyUlXVlHPwk5"
      },
      "source": [
        "# load_data= '/content/drive/My Drive/Courses/NLP/Project/data'\n",
        "# train_df= pd.read_csv(join(load_data,'train.csv'))\n",
        "# train_df.columns =['caption', 'MSR', 'test','id', 'label','sentence_1','sentence_2','url','url_2']\n",
        "# dev_df= pd.read_csv(join(load_data,'dev.csv'))\n",
        "# dev_df.columns =['caption', 'MSR', 'test','id', 'label','sentence_1','sentence_2','url','url_2']\n",
        "# test_df= pd.read_csv(join(load_data,'test.csv'))\n",
        "# #test_df = new_df\n",
        "# test_df.columns =['caption', 'MSR', 'test','id', 'label','sentence_1','sentence_2','url','url_2']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRcCbBjwZyYr"
      },
      "source": [
        "# #Model Intialization\n",
        "\n",
        "# #epochs=10\n",
        "\n",
        "# #Load Model\n",
        "# model_path  = '/content/drive/My Drive/Courses/NLP/Project/model'\n",
        "# model= Regressor(model_path)\n",
        "# weights_score = torch.load(join(model_path,'model_state.bin'))\n",
        "# model.out.load_state_dict(weights_score)\n",
        "# model.to(device)\n",
        "\n",
        "# #To tokenize  the data\n",
        "# #tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# # # Prepare optimizer\n",
        "# # optimizer = AdamW(model.parameters(),lr=2e-5)\n",
        "\n",
        "# # #Loss Function\n",
        "# # mse_loss= nn.MSELoss().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61GBl4kBRXWr"
      },
      "source": [
        "# tokenizer = BertTokenizer.from_pretrained('/content/drive/My Drive/Courses/NLP/Project/model')\n",
        "# #tokenizer = BertTokenizer.from_pretrained('/content/drive/My Drive/Courses/NLP/Project/model')\n",
        "# # test_dataloader = create_dataloader(tokenizer, train_df)\n",
        "# # train_dataloader = create_dataloader(tokenizer, train_df)\n",
        "# # dev_dataloader = create_dataloader(tokenizer, dev_df)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}